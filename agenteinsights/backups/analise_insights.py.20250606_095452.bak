"""
Agente Insights - An√°lises e Chat IA
====================================
Vers√£o: 1.5.0
Release: 5
Data: 02/06/2025

Descri√ß√£o:
Executa an√°lises (descritiva, preditiva, diagn√≥stica) e fornece fun√ß√£o para chat IA,
onde o usu√°rio pode consultar insights sobre tribos/squads, com respostas baseadas nos dados,
gr√°ficos e tabelas. Permite salvar o hist√≥rico do chat em DOCX.
"""

import pandas as pd
import numpy as np
import os
import logging
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from docx import Document
from datetime import datetime
from typing import Dict, Any, List, Tuple
from dotenv import load_dotenv
import openai
from openai import OpenAI  # Adicione esta linha junto com os outros imports
import unicodedata
from collections import Counter
import json
import traceback
from .config import *
import re
from unidecode import unidecode
from pathlib import Path

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Definir caminhos
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # Sobe um n√≠vel
DATA_DIR = os.path.join(BASE_DIR, 'dados')  # Usa 'dados' em vez de 'data'
OUTPUT_DIR = os.path.join(BASE_DIR, 'output')
GRAFICOS_DIR = os.path.join(OUTPUT_DIR, 'graficos')
RELATORIOS_DIR = os.path.join(OUTPUT_DIR, 'relatorios')

# Criar diret√≥rios necess√°rios
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(GRAFICOS_DIR, exist_ok=True)
os.makedirs(RELATORIOS_DIR, exist_ok=True)

# Caminhos para arquivos
ARQUIVO_MATURIDADE = os.path.join(DATA_DIR, 'MaturidadeT.xlsx')
ARQUIVO_ALOCACAO = os.path.join(DATA_DIR, 'Alocacao.xlsx')
ARQUIVO_EXECUTIVO = os.path.join(DATA_DIR, 'Executivo.xlsx')

def normalizar_coluna(col):
    # Remove acentos, espa√ßos e deixa min√∫sculo
    col = unicodedata.normalize('NFKD', str(col)).encode('ASCII', 'ignore').decode('ASCII')
    return col.strip().lower().replace(' ', '')

def mapear_colunas(df, nomes_esperados):
    # Cria um dicion√°rio de {nome_normalizado: nome_original}
    col_map = {normalizar_coluna(c): c for c in df.columns}
    resultado = {}
    for nome in nomes_esperados:
        norm = normalizar_coluna(nome)
        if norm in col_map:
            resultado[nome] = col_map[norm]
        else:
            resultado[nome] = None
    return resultado

def carregar_dados():
    """Carrega os dados dos arquivos Excel"""
    try:
        dados = {
            'maturidade': pd.read_excel(ARQUIVO_MATURIDADE),
            'alocacao': pd.read_excel(ARQUIVO_ALOCACAO),
            'executivo': pd.read_excel(ARQUIVO_EXECUTIVO)
        }
        
        # Log das colunas dispon√≠veis
        for nome, df in dados.items():
            logging.debug(f"Colunas em {nome}: {df.columns.tolist()}")
            
        return dados
        
    except Exception as e:
        logging.error(f"Erro ao carregar dados: {str(e)}")
        raise

def padronizar_ids(dados):
    """Padroniza os nomes das tribos para permitir o merge correto"""
    logging.info("Padronizando nomes das tribos para merge...")
    
    try:
        # Verificar se dados √© um DataFrame ou um dicion√°rio com DataFrames
        if isinstance(dados, pd.DataFrame):
            # Se for um √∫nico DataFrame, aplicar a normaliza√ß√£o diretamente
            df = dados.copy()
            
            # Fun√ß√£o para limpar e padronizar nomes
            def limpar_nome(nome):
                if pd.isna(nome):
                    return ''
                nome = str(nome).lower().strip()
                # Remover acentos
                nome = unidecode(nome)
                # Remover caracteres especiais
                nome = re.sub(r'[^a-z0-9\s]', '', nome)
                # Substituir m√∫ltiplos espa√ßos por um √∫nico
                nome = re.sub(r'\s+', ' ', nome)
                return nome.strip()
            
            # Verificar quais colunas podem conter nomes de tribos
            colunas_tribo = [col for col in df.columns if 'tribe' in col.lower() or 'tribo' in col.lower()]
            
            # Aplicar normaliza√ß√£o nas colunas identificadas
            for col in colunas_tribo:
                norm_col = f"{col}_norm"
                df[norm_col] = df[col].apply(limpar_nome)
                logging.info(f"Padronizado coluna {col} -> {norm_col}")
                
            return df
        else:
            # Caso original: √© um dicion√°rio com DataFrames
            # Criar c√≥pias para n√£o modificar os originais
            maturidade = dados['maturidade'].copy() if 'maturidade' in dados else pd.DataFrame()
            alocacao = dados['alocacao'].copy() if 'alocacao' in dados else pd.DataFrame()
            
            # Fun√ß√£o para limpar e padronizar nomes
            def limpar_nome(nome):
                if pd.isna(nome):
                    return ''
                nome = str(nome).lower().strip()
                # Remover acentos
                nome = unidecode(nome)
                # Remover caracteres especiais
                nome = re.sub(r'[^a-z0-9\s]', '', nome)
                # Substituir m√∫ltiplos espa√ßos por um √∫nico
                nome = re.sub(r'\s+', ' ', nome)
                return nome.strip()

            # Padronizar nomes em Maturidade se existir e tiver a coluna 'Tribo'
            if not maturidade.empty and 'Tribo' in maturidade.columns:
                maturidade['nome_tribo_clean'] = maturidade['Tribo'].apply(limpar_nome)
                logging.info(f"Nomes √∫nicos em Maturidade: {maturidade['nome_tribo_clean'].unique().tolist()}")
            
            # Padronizar nomes em Aloca√ß√£o se existir e tiver a coluna 'tribe'
            if not alocacao.empty and 'tribe' in alocacao.columns:
                alocacao['nome_tribo_clean'] = alocacao['tribe'].apply(limpar_nome)
                logging.info(f"Nomes √∫nicos em Aloca√ß√£o: {alocacao['nome_tribo_clean'].unique().tolist()}")
            
            # Verificar nomes em comum se ambos os DataFrames t√™m as colunas necess√°rias
            if (not maturidade.empty and 'nome_tribo_clean' in maturidade.columns and 
                not alocacao.empty and 'nome_tribo_clean' in alocacao.columns):
                nomes_maturidade = set(maturidade['nome_tribo_clean'].unique())
                nomes_alocacao = set(alocacao['nome_tribo_clean'].unique())
                nomes_comuns = nomes_maturidade.intersection(nomes_alocacao)
                
                logging.info(f"Total de nomes em comum: {len(nomes_comuns)}")
                logging.info(f"Nomes em comum: {nomes_comuns}")
            
            # Atualizar DataFrames no dicion√°rio
            dados['maturidade'] = maturidade
            dados['alocacao'] = alocacao
            
            return dados
        
    except Exception as e:
        logging.error(f"Erro ao padronizar nomes: {str(e)}")
        raise

def cruzar_dados(dados):
    """Cruza os dados dos diferentes DataFrames"""
    try:
        logging.info("Iniciando cruzamento de dados...")
        
        # Padronizar IDs primeiro
        dados_padronizados = padronizar_ids(dados)
        
        # Log das colunas antes do merge
        for nome, df in dados_padronizados.items():
            logging.info(f"Colunas em {nome}: {df.columns.tolist()}")
          # Merge usando as colunas padronizadas com nomes limpos
        df_merged = dados_padronizados['maturidade'].merge(
            dados_padronizados['alocacao'],
            left_on='nome_tribo_clean',
            right_on='nome_tribo_clean',
            how='inner',
            suffixes=('_mat', '_aloc')
        )
        
        logging.info(f"Merge realizado com sucesso. Shape: {df_merged.shape}")
        
        # Validar resultado
        if df_merged.empty:
            raise ValueError("Merge resultou em DataFrame vazio")
              # Log dos primeiros registros para verifica√ß√£o
        logging.info("\nPrimeiros registros ap√≥s merge:")
        logging.info(f"\n{df_merged[['nome_tribo_clean', 'Tribo', 'Maturidade']].head()}")
            
        return df_merged
        
    except Exception as e:
        logging.error(f"Erro no cruzamento de dados: {str(e)}")
        raise

def executar_analises(df: pd.DataFrame) -> Dict[str, Any]:
    resultados = {}
    # An√°lise descritiva
    resultados['estatisticas'] = df.describe(include='all').to_dict()
    # An√°lise preditiva (regress√£o)
    num_cols = df.select_dtypes(include=[np.number]).columns
    if len(num_cols) > 1:
        X = df[num_cols[1:]].fillna(0)
        y = df[num_cols[0]].fillna(0)
        reg = LinearRegression().fit(X, y)
        resultados['regressao'] = {
            'coef': reg.coef_.tolist(),
            'intercept': float(reg.intercept_),
            'r2': reg.score(X, y)
        }
    else:
        resultados['regressao'] = None
    # An√°lise diagn√≥stica (clustering)
    if len(num_cols) > 1:
        X = df[num_cols].fillna(0)
        kmeans = KMeans(n_clusters=3, random_state=42).fit(X)
        resultados['clustering'] = {
            'labels': kmeans.labels_.tolist(),
            'centroids': kmeans.cluster_centers_.tolist(),
            'inertia': float(kmeans.inertia_)
        }
    else:
        resultados['clustering'] = None
    resultados['status'] = 'success'
    return resultados

def gerar_graficos(df: pd.DataFrame, resultados: Dict[str, Any]):
    # Exemplo: histograma da primeira coluna num√©rica
    num_cols = df.select_dtypes(include=[np.number]).columns
    if len(num_cols) > 0:
        plt.figure(figsize=(8, 4))
        sns.histplot(df[num_cols[0]].dropna())
        plt.title(f'Histograma de {num_cols[0]}')
        plt.savefig('output/graficos/histograma.png')
        plt.close()

def chat_ia_loop(analises: Dict[str, Any]):
    """Chat IA com suporte a consultas din√¢micas sobre estrutura organizacional"""
    try:
        load_dotenv()
        client = OpenAI()
        
        # Inicializar contexto da conversa
        contexto_conversa = []
        
        # Mapear estrutura organizacional
        estrutura = mapear_estrutura_org(analises)
        
        # Validar estrutura
        if not estrutura['tribos']:
            msg_erro = "N√£o foi poss√≠vel carregar a estrutura organizacional. Verificar dados de entrada."
            logging.error(msg_erro)
            print(f"\n‚ùå {msg_erro}")
            return
        
        # Preparar prompt base com informa√ß√µes da estrutura
        sistema_base = {
            "role": "system",
            "content": f"""Voc√™ √© um consultor s√™nior especializado em Business Agility e Analytics.
            
            # Estrutura Organizacional
            - Total de Tribos: {len(estrutura['tribos'])}
            - Total de Squads: {estrutura['total_squads']}
            - Total de Pessoas: {estrutura['total_pessoas']}
            
            # Tribos Dispon√≠veis
            {chr(10).join(f"- {tribo}" for tribo in estrutura['tribos'].keys())}
            
            # Pap√©is na Organiza√ß√£o
            {chr(10).join(f"- {papel}: {qtd}" for papel, qtd in estrutura.get('papeis_total', {}).items())}            """
        }
        
        print("\nüìä Consultor Executivo - Business Agility & Analytics")
        print(f"\nEstrutura Atual:")
        print(f"- {len(estrutura['tribos'])} Tribos")
        print(f"- {estrutura['total_squads']} Squads")
        print(f"- {estrutura['total_pessoas']} Pessoas")
        print("\nExemplos de consultas:")
        print("- 'an√°lise da tribo [nome da tribo]'")
        print("- 'composi√ß√£o do squad [nome do squad]'")
        print("- 'distribui√ß√£o de pap√©is na tribo [nome da tribo]'")
        print("- 'm√©tricas da organiza√ß√£o'")
        print("\nDigite 'sair' para encerrar")
        
        max_erros_consecutivos = 0
        max_erros_totais = 0
        while True:
            try:
                # Mecanismo de recupera√ß√£o para evitar loops infinitos
                if max_erros_consecutivos >= 3 or max_erros_totais >= 5:
                    logging.error(f"Muitos erros na entrada. Consecutivos: {max_erros_consecutivos}, Totais: {max_erros_totais}. Encerrando chat.")
                    print("\n‚ùå Muitos erros de entrada detectados. Encerrando o chat.")
                    break
                
                try:
                    print("\nVoc√™: ", end='', flush=True)
                    query = input().strip()
                    max_erros_consecutivos = 0  # Reset contador de erros consecutivos ap√≥s entrada bem-sucedida
                    
                    # Log de sucesso na leitura de entrada
                    logging.debug("Entrada do usu√°rio lida com sucesso")
                    
                except EOFError:
                    logging.warning("Erro EOF ao ler entrada. Tentando recuperar...")
                    max_erros_consecutivos += 1
                    max_erros_totais += 1
                    print("\n‚ö†Ô∏è Erro ao ler entrada (EOF). Digite novamente ou 'sair' para encerrar.")
                    # Pequena pausa para evitar ciclo muito r√°pido
                    import time
                    time.sleep(0.5)
                    continue
                
                if query.lower() == "sair":
                    break
                
                # Log da consulta recebida
                logging.info(f"Consulta recebida: '{query}'")
                
                # Identificar entidade (tribo/squad) na query
                entidade, nome = identificar_entidade_consulta(query, estrutura)
                logging.info(f"Entidade identificada: '{entidade}', Nome: '{nome}'")
                
                # Preparar dados espec√≠ficos da consulta
                dados_consulta = preparar_dados_consulta(entidade, nome, estrutura, analises)
                
                # Gerar resposta contextualizada
                resposta = gerar_resposta_contextualizada(query, entidade, dados_consulta, client)
                
                # Atualizar contexto da conversa
                contexto_conversa.append(("Voc√™", query))
                contexto_conversa.append(("IA", resposta))
                
                print(f"\nü§ñ IA: {resposta}\n")
                
            except Exception as e:
                logging.error(f"Erro no chat: {str(e)}")
                traceback.print_exc()
                print(f"\n‚ùå Ops! Tive um problema: {str(e)}")
        
        return contexto_conversa
    except Exception as e:
        logging.error(f"Erro ao iniciar chat: {str(e)}")
        traceback.print_exc()
        print(f"\n‚ùå Erro ao iniciar chat: {str(e)}")

def salvar_chat_docx(chat_log: List[tuple]):
    doc = Document()
    doc.add_heading('Chat de Insights - Agente Insights', 0)
    for autor, msg in chat_log:
        doc.add_paragraph(f"{autor}:", style='Heading 2')
        doc.add_paragraph(msg)
    caminho = f"output/relatorios/chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
    doc.save(caminho)
    print(f"Chat salvo em: {caminho}")

def analisar_alocacao(dados: pd.DataFrame, tribo: str = None, squad: str = None) -> Dict:
    """Analisa aloca√ß√£o de pessoas e pap√©is"""
    try:
        # Criar c√≥pia dos dados
        df = dados.copy() if not dados.empty else pd.DataFrame()
        
        # Verificar colunas necess√°rias base (sem coluna de percentual)
        colunas_necessarias_base = ['endDate', 'role', 'squad', 'tribe', 'person']
        
        # Verificar varia√ß√µes da coluna de aloca√ß√£o percentual
        coluna_percentual = None
        for possivel_coluna in ['percentageAllocation', 'percetageAllocation', 'percentage', 'alocacao_percentual']:
            if possivel_coluna in df.columns:
                coluna_percentual = possivel_coluna
                break
        
        # Log da coluna de percentual encontrada
        if coluna_percentual:
            logging.info(f"Coluna de percentual encontrada: {coluna_percentual}")
        else:
            logging.warning("Nenhuma coluna de percentual encontrada")
            
        # Verificar se temos as colunas base necess√°rias
        if not all(col in df.columns for col in colunas_necessarias_base):
            colunas_faltando = [col for col in colunas_necessarias_base if col not in df.columns]
            logging.warning(f"Colunas base ausentes para an√°lise de aloca√ß√£o: {colunas_faltando}")
            return {
                'papeis': {},
                'alocacao_media': {},
                'pessoas_multi_squad': [],
                'composicao_squads': {},
                'media_pessoas_squad': 0
            }
        
        # Filtrar apenas aloca√ß√µes ativas (sem data de t√©rmino ou data futura)
        df = df[df['endDate'].isna() | (pd.to_datetime(df['endDate'], errors='coerce') > pd.Timestamp.now())]
        
        # Filtrar por tribo ou squad se especificado
        if tribo:
            df = df[df['tribe'] == tribo]
        if squad:
            df = df[df['squad'] == squad]
            
        # Log dos registros ap√≥s filtros
        logging.info(f"Registros ap√≥s filtros de tribo/squad: {len(df)}")

        # Preparar dados para an√°lise
        composicao_squads_agg = {}
        
        # Base para an√°lise sem percentual
        composicao_squads_agg['role'] = lambda x: dict(Counter(x))
        
        # Adicionar percentual se dispon√≠vel, com convers√£o segura para num√©rico
        if coluna_percentual:
            # Converter percentuais para valores num√©ricos com seguran√ßa
            try:
                # Se for string com %, remover e converter para float
                if df[coluna_percentual].dtype == 'object':
                    logging.info(f"Processando coluna {coluna_percentual}")
                    # Mostrar algumas amostras para debug
                    sample_values = df[coluna_percentual].dropna().head(5).tolist()
                    logging.info(f"Amostras de valores antes da convers√£o: {sample_values}")
                    
                    # Limpar e converter valores
                    df[coluna_percentual] = df[coluna_percentual].astype(str).str.replace('%', '').str.replace(',', '.')
                    df[coluna_percentual] = pd.to_numeric(df[coluna_percentual], errors='coerce')
                    
                    # Mostrar valores ap√≥s limpeza para debug
                    clean_values = df[coluna_percentual].dropna().head(3).tolist()
                    logging.info(f"Valores ap√≥s limpeza: {clean_values}")
                    
                    # Verificar valores inv√°lidos (que viraram NaN)
                    invalid_count = df[coluna_percentual].isna().sum()
                    logging.info(f"Dados inv√°lidos em {coluna_percentual}: {invalid_count} registros")
                
                # Calcular estat√≠sticas b√°sicas
                if df[coluna_percentual].notna().any():
                    min_val = df[coluna_percentual].min()
                    max_val = df[coluna_percentual].max()
                    mean_val = df[coluna_percentual].mean()
                    logging.info(f"Valores de percentual: min={min_val}, max={max_val}, mean={mean_val}")
                    
                    # Converter para decimal (0-1) se os valores parecem ser percentuais (>1)
                    if max_val > 1:
                        logging.info(f"Convertendo percentuais para decimal (0-1)")
                        df[coluna_percentual] = df[coluna_percentual] / 100
                        logging.info(f"Valores ap√≥s normaliza√ß√£o: min={df[coluna_percentual].min()}, max={df[coluna_percentual].max()}, mean={df[coluna_percentual].mean()}")
                
                # Usar mean para agrega√ß√£o apenas se os valores s√£o num√©ricos
                composicao_squads_agg[coluna_percentual] = 'mean'
                
            except Exception as e:
                logging.warning(f"Erro ao processar coluna percentual {coluna_percentual}: {str(e)}")
                # Remover a coluna problem√°tica
                if coluna_percentual in df.columns:
                    df = df.drop(coluna_percentual, axis=1)
                coluna_percentual = None
                
        # Calcular m√©tricas de aloca√ß√£o
        analise = {
            'papeis': df.groupby('role').size().to_dict() if len(df) > 0 else {},
            'pessoas_multi_squad': df[df.groupby('person')['squad'].transform('size') > 1]['person'].unique().tolist() if len(df) > 0 else [],
            'media_pessoas_squad': float(df.groupby('squad').size().mean()) if len(df) > 0 and 'squad' in df.columns else 0
        }
            
        # Adicionar m√©tricas de aloca√ß√£o percentual se dispon√≠vel
        if coluna_percentual and coluna_percentual in df.columns:
            analise['alocacao_media'] = df.groupby('squad')[coluna_percentual].mean().to_dict() if len(df) > 0 else {}
            
            # Usar agrega√ß√£o segura para composi√ß√£o de squads
            try:
                analise['composicao_squads'] = df.groupby('squad').agg(composicao_squads_agg).to_dict() if len(df) > 0 else {}
            except Exception as e:
                logging.warning(f"Erro na agrega√ß√£o de composi√ß√£o de squads: {str(e)}")
                # Fallback para composi√ß√£o sem percentual
                analise['composicao_squads'] = df.groupby('squad').agg({'role': lambda x: dict(Counter(x))}).to_dict() if len(df) > 0 else {}
        else:
            analise['alocacao_media'] = {}
            analise['composicao_squads'] = df.groupby('squad').agg({'role': lambda x: dict(Counter(x))}).to_dict() if len(df) > 0 else {}
        
        return analise
        
    except Exception as e:
        logging.error(f"Erro ao analisar aloca√ß√£o: {str(e)}")
        traceback.print_exc()
        return {
            'papeis': {},
            'alocacao_media': {},
            'pessoas_multi_squad': [],
            'composicao_squads': {},
            'media_pessoas_squad': 0
        }

def mapear_colunas_ageis(df):
    """Mapeia colunas do dataframe para nomes padronizados"""
    # Mapeamento expandido com mais varia√ß√µes comuns
    mapeamento = {
        'lead_time': ['LeadTime', 'lead_time', 'leadtime', 'tempo_total', 'lead time', 'tempo total'],
        'cycle_time': ['CycleTime', 'cycle_time', 'cycletime', 'tempo_ciclo', 'cycle time', 'tempo ciclo'],
        'data': ['DataRegistro', 'data_registro', 'data', 'date', 'created_at', 'data_criacao'],
        'issue_type': ['TipoItem', 'tipo_item', 'type', 'issue_type', 'tipo', 'tipo_tarefa'],
        'status': ['Status', 'status', 'estado', 'situacao'],
        'issue_id': ['ID', 'id', 'numero', 'issue_key', 'chave'],
        'squad': ['Squad', 'squad', 'team', 'equipe'],
        'tribo': ['Tribe', 'tribo', 'Chave_Tribo_Ano_Quarter', 'tribe', 'tribo_nome']
    }
    
    colunas_encontradas = {}
    
    # Primeiro tenta match exato
    for nova_col, possiveis_nomes in mapeamento.items():
        for nome in possiveis_nomes:
            if nome in df.columns:
                colunas_encontradas[nova_col] = nome
                logging.info(f"Match exato encontrado para {nova_col}: {nome}")
                break
    
    # Se n√£o encontrou todas, tenta match parcial case-insensitive
    for nova_col, possiveis_nomes in mapeamento.items():
        if nova_col not in colunas_encontradas:
            for nome in possiveis_nomes:
                matches = [col for col in df.columns if nome.lower() in col.lower()]
                if matches:
                    colunas_encontradas[nova_col] = matches[0]
                    logging.info(f"Match parcial encontrado para {nova_col}: {matches[0]}")
                    break
    
    # Log do resultado final
    logging.info(f"Mapeamento final de colunas: {colunas_encontradas}")
    logging.info(f"Colunas n√£o mapeadas: {[col for col in mapeamento.keys() if col not in colunas_encontradas]}")
    
    return colunas_encontradas

def analisar_metricas_ageis(df, tribo: str = None):
    """Analisa m√©tricas √°geis do dataframe"""
    try:
        # Mapeia as colunas primeiro para garantir que temos os nomes corretos
        colunas = mapear_colunas_ageis(df)
        if not colunas:
            raise ValueError("N√£o foi poss√≠vel mapear as colunas necess√°rias")
            
        # Filtrar por tribo se especificado
        if tribo and 'tribo' in colunas:
            # Primeiro normalizamos o nome da tribo para compara√ß√£o
            tribo_norm = normalizar_texto(tribo)
            # Criamos uma coluna tempor√°ria normalizada para compara√ß√£o
            df['tribo_temp_norm'] = df[colunas['tribo']].apply(normalizar_texto)
            # Filtramos usando a coluna normalizada
            df = df[df['tribo_temp_norm'] == tribo_norm]
            # Removemos a coluna tempor√°ria
            df = df.drop('tribo_temp_norm', axis=1)
            
            logging.info(f"Dados filtrados para tribo '{tribo}': {len(df)} registros")
            
        # Analisa CFD
        cfd_metricas = analisar_cfd(df, colunas)
        if not cfd_metricas:
            raise ValueError("N√£o foi poss√≠vel analisar o CFD")
            
        # Calcula m√©tricas por tribo
        metricas_por_tribo = {}
        if 'tribo' in colunas:
            tribos = df[colunas['tribo']].unique()
            for tribo in tribos:
                df_tribo = df[df[colunas['tribo']] == tribo]
                metricas_tribo = analisar_cfd(df_tribo, colunas)
                if metricas_tribo:
                    # Calcula taxa de bugs
                    if 'issue_type' in colunas:
                        total_issues = len(df_tribo)
                        bugs = len(df_tribo[df_tribo[colunas['issue_type']].str.contains('bug', case=False, na=False)])
                        bug_ratio = bugs / total_issues if total_issues > 0 else None
                    else:
                        bug_ratio = None
                    
                    metricas_por_tribo[tribo] = {
                        'throughput': metricas_tribo['throughput'],
                        'throughput_diario': metricas_tribo['throughput_diario'],
                        'lead_time': metricas_tribo['avg_lead_time'],
                        'wip': metricas_tribo['wip'],
                        'bug_ratio': bug_ratio
                    }
                    
        # Calcula m√©tricas por squad
        metricas_por_squad = {}
        if 'squad' in colunas:
            squads = df[colunas['squad']].unique()
            for squad in squads:
                df_squad = df[df[colunas['squad']] == squad]
                metricas_squad = analisar_cfd(df_squad, colunas)
                if metricas_squad:
                    # Calcula taxa de bugs
                    if 'issue_type' in colunas:
                        total_issues = len(df_squad)
                        bugs = len(df_squad[df_squad[colunas['issue_type']].str.contains('bug', case=False, na=False)])
                        bug_ratio = bugs / total_issues if total_issues > 0 else None
                    else:
                        bug_ratio = None
                    
                    metricas_por_squad[squad] = {
                        'throughput': metricas_squad['throughput'],
                        'throughput_diario': metricas_squad['throughput_diario'],
                        'lead_time': metricas_squad['avg_lead_time'],
                        'wip': metricas_squad['wip'],
                        'bug_ratio': bug_ratio
                    }
        
        return {
            'metricas_gerais': {
                'throughput': cfd_metricas['throughput'],
                'throughput_diario': cfd_metricas['throughput_diario'],
                'lead_time': cfd_metricas['avg_lead_time'],
                'wip': cfd_metricas['wip']
            },
            'metricas_por_tribo': metricas_por_tribo,
            'metricas_por_squad': metricas_por_squad
        }
        
    except Exception as e:
        logging.error(f"Erro ao analisar m√©tricas √°geis: {str(e)}")
        return None

def analisar_cfd(df, colunas):
    """Analisa o Cumulative Flow Diagram (CFD) para calcular m√©tricas importantes"""
    try:
        # Verifica se temos as colunas necess√°rias
        if not {'data'}.issubset(colunas.keys()):
            logging.error("Coluna de data para CFD n√£o encontrada")
            # Retornar m√©tricas padr√£o para n√£o quebrar o fluxo
            return {
                'wip': 0,
                'throughput': 0,
                'throughput_diario': 0,
                'avg_lead_time': None,
                'cfd_data': pd.DataFrame()
            }
        
        # Se n√£o tem status, criamos um status artificial para n√£o quebrar o fluxo
        if 'status' not in colunas:
            logging.warning("Coluna de status n√£o encontrada, usando status artificial")
            df['status_artificial'] = 'Em andamento'  # Status gen√©rico
            colunas['status'] = 'status_artificial'

        # Ordena por data
        df_sorted = df.sort_values(by=colunas['data'])
        
        # Agrupa por data e status e conta os itens
        cfd_data = df_sorted.groupby([colunas['data'], colunas['status']]).size().unstack(fill_value=0)
        
        # Calcula o acumulado para cada status
        cfd_cumsum = cfd_data.cumsum()
        
        # Calcula m√©tricas baseadas no CFD
        wip = cfd_cumsum.iloc[-1].sum() if not cfd_cumsum.empty else 0  # Work in Progress atual
        
        # Calcula throughput com prote√ß√£o contra divis√£o por zero
        try:
            # Calcula a diferen√ßa de dias
            dias_diff = (df_sorted[colunas['data']].max() - df_sorted[colunas['data']].min()).days
            if dias_diff > 0:
                # Total de itens conclu√≠dos
                itens_concluidos = len(df[df[colunas['status']].isin(['Done', 'Conclu√≠do', 'Completed'])])
                
                # Throughput di√°rio (itens por dia)
                throughput_diario = itens_concluidos / dias_diff if dias_diff > 0 else 0
                
                # Throughput mensal (itens por m√™s)
                throughput = itens_concluidos / (dias_diff / 30) if dias_diff > 0 else 0
            else:
                throughput_diario = 0
                throughput = 0  # caso n√£o haja diferen√ßa de dias
        except Exception as e:
            logging.warning(f"Erro ao calcular throughput: {str(e)}")
            throughput_diario = 0
            throughput = 0  # valor padr√£o em caso de erro
        
        # Calcula o lead time m√©dio se dispon√≠vel
        if 'lead_time' in colunas:
            avg_lead_time = df[df[colunas['status']].isin(['Done', 'Conclu√≠do', 'Completed'])][colunas['lead_time']].mean()
        else:
            avg_lead_time = None
        
        return {
            'wip': wip,
            'throughput': throughput,
            'throughput_diario': throughput_diario,
            'avg_lead_time': avg_lead_time,
            'cfd_data': cfd_cumsum
        }
        
    except Exception as e:
        logging.error(f"Erro ao analisar CFD: {str(e)}")
        return None

def gerar_insights(metricas: Dict, alocacao: Dict, cfd: Dict) -> List[str]:
    """Gera insights estrat√©gicos baseados nas an√°lises"""
    insights = []
    
    # Verificar se temos dados v√°lidos
    if not all(isinstance(x, dict) for x in [metricas, alocacao, cfd]):
        return ["‚ö†Ô∏è **Dados Insuficientes**: N√£o foi poss√≠vel gerar insights."]
    
    # An√°lise de Capacidade vs Demanda
    taxa_saida = cfd.get('throughput', 0)
    taxa_entrada = cfd.get('wip', 0)
    taxa_utilizacao = taxa_saida / taxa_entrada if taxa_entrada > 0 else 0
    if taxa_utilizacao < 0.8:
        insights.append(f"üö® **Alerta de Capacidade**: Taxa de utiliza√ß√£o em {taxa_utilizacao:.1%}. Backlog crescente pode impactar entregas futuras.")
    
    # An√°lise de Qualidade e Sustentabilidade
    if metricas['qualidade']['bugs_ratio'] > 0.2:
        insights.append(f"‚ö†Ô∏è **Risco de Qualidade**: Taxa de bugs em {metricas['qualidade']['bugs_ratio']:.1%} pode impactar satisfa√ß√£o do cliente e custo de manuten√ß√£o.")
    
    # An√°lise de Composi√ß√£o de Times
    gaps_squads = []
    for squad, papeis in alocacao['composicao_squads'].items():
        gaps = []
        if 'QA' not in papeis:
            gaps.append('QA')
        if 'Tech Lead' not in papeis:
            gaps.append('Tech Lead')
        if gaps:
            gaps_squads.append(f"Squad {squad}: {', '.join(gaps)}")
    
    if gaps_squads:
        insights.append(f"üë• **Gap de Pap√©is Cr√≠ticos**: Identificados gaps que podem impactar qualidade e lideran√ßa t√©cnica:\n" + "\n".join(gaps_squads))
    
    # An√°lise de Efici√™ncia
    if metricas['lead_time']['media'] > 14:  # Exemplo de threshold
        insights.append(f"‚è±Ô∏è **Oportunidade de Otimiza√ß√£o**: Lead time m√©dio de {metricas['lead_time']['media']:.1f} dias indica potencial para melhorias no fluxo de valor.")
    
    return insights

def gerar_insights_detalhados(metricas: Dict, alocacao: Dict, cfd: Dict) -> List[str]:
    """Gera insights estrat√©gicos detalhados baseados nas an√°lises"""
    insights = []
    
    # Verifica√ß√£o robusta de dados
    if not metricas or not isinstance(metricas, dict) or not alocacao or not isinstance(alocacao, dict) or not cfd or not isinstance(cfd, dict):
        return ["‚ö†Ô∏è **Dados Insuficientes**: N√£o foi poss√≠vel gerar insights detalhados."]
    
    # Verificar os dicion√°rios com defaults seguros
    metricas_gerais = metricas.get('metricas_gerais', {})
    if not isinstance(metricas_gerais, dict):
        metricas_gerais = {}
        
    # An√°lise de Maturidade - com valida√ß√µes robustas
    maturidade_media = metricas_gerais.get('maturidade_media', 0)
    maturidade_std = metricas_gerais.get('maturidade_std', 0)
    
    if maturidade_media >= 85:
        insights.append("üìà **Maturidade Excelente**: A organiza√ß√£o apresenta n√≠vel excelente de maturidade √°gil, indicando pr√°ticas bem estabelecidas e consistentes.")
    elif maturidade_media >= 75:
        insights.append("üìä **Maturidade Alta**: Boas pr√°ticas implementadas, com oportunidades pontuais de aprimoramento.")
    elif maturidade_media >= 60:
        insights.append("‚ö†Ô∏è **Maturidade M√©dia**: Pr√°ticas √°geis em desenvolvimento, necessitando fortalecimento.")
    else:
        insights.append("üö® **Maturidade Baixa**: Pr√°ticas √°geis em est√°gio inicial, requerendo aten√ß√£o priorit√°ria.")

    # An√°lise de Variabilidade
    if maturidade_std > 15:
        insights.append(f"üìä **Alta Variabilidade**: Inconsist√™ncia significativa na ado√ß√£o de pr√°ticas √°geis (œÉ={maturidade_std:.2f}).")
    elif maturidade_std > 10:
        insights.append(f"üìà **Variabilidade Moderada**: Algumas √°reas mais avan√ßadas que outras (œÉ={maturidade_std:.2f}).")
    else:
        insights.append(f"‚úÖ **Baixa Variabilidade**: Consist√™ncia na ado√ß√£o de pr√°ticas √°geis (œÉ={maturidade_std:.2f}).")

    # An√°lise de Aloca√ß√£o - com tratamento para valores nulos
    media_pessoas_squad = alocacao.get('media_pessoas_squad', 0)
    if media_pessoas_squad > 15:
        insights.append(f"‚ö†Ô∏è **Equipes Grandes**: M√©dia de {media_pessoas_squad:.1f} pessoas por squad, acima do recomendado (5-9).")
    elif media_pessoas_squad < 5:
        insights.append(f"‚ö†Ô∏è **Equipes Pequenas**: M√©dia de {media_pessoas_squad:.1f} pessoas por squad, abaixo do recomendado (5-9).")
        
    # An√°lise de Efici√™ncia - evitando divis√£o por zero
    taxa_saida = cfd.get('throughput', 0)
    taxa_entrada = cfd.get('wip', 1)  # Usar 1 como valor padr√£o para evitar divis√£o por zero
    if taxa_entrada > 0:
        taxa_utilizacao = taxa_saida / taxa_entrada
        if taxa_utilizacao < 0.8:
            insights.append(f"üö® **Gargalo de Entrega**: Taxa de utiliza√ß√£o em {taxa_utilizacao:.1%}, indicando ac√∫mulo de backlog.")
      
    # An√°lise de Qualidade - se a informa√ß√£o estiver dispon√≠vel
    qualidade_metricas = metricas.get('qualidade', {})
    if isinstance(qualidade_metricas, dict):
        bugs_ratio = qualidade_metricas.get('bugs_ratio', 0)
        if bugs_ratio > 0.2:
            insights.append(f"‚ö†Ô∏è **Risco de Qualidade**: Taxa de bugs em {bugs_ratio:.1%}.")
    
    # An√°lise de Lead Time - se a informa√ß√£o estiver dispon√≠vel
    lead_time_metricas = metricas.get('lead_time', {})
    if isinstance(lead_time_metricas, dict):
        lead_time = lead_time_metricas.get('media', 0)
        if lead_time > 14:
            insights.append(f"‚è±Ô∏è **Lead Time Alto**: M√©dia de {lead_time:.1f} dias, indicando oportunidades de otimiza√ß√£o.")

    # An√°lise de Portfolio - validando presen√ßa de dados
    if isinstance(alocacao.get('composicao_squads', {}), dict) and alocacao.get('composicao_squads'):
        squads_grandes = sum(1 for _, papeis in alocacao['composicao_squads'].items() 
                           if isinstance(papeis, dict) and len(papeis) > 9)
        if squads_grandes > 0:
            insights.append(f"üìä **Distribui√ß√£o Inadequada**: {squads_grandes} squads com mais de 9 pessoas.")

    # Se n√£o conseguiu gerar insights espec√≠ficos
    if not insights:
        insights.append("‚ÑπÔ∏è **An√°lise Limitada**: N√£o foi poss√≠vel gerar insights espec√≠ficos com os dados dispon√≠veis.")

    return insights

def extrair_metricas_ageis(analises: Dict[str, Any], tribo: str = None) -> Dict:
    """Extrai e processa m√©tricas √°geis com valida√ß√£o de dados"""
    try:
        df = analises.get('dados_cruzados', pd.DataFrame())
        if df.empty:
            logging.warning("Dados cruzados n√£o encontrados ou vazios")
            return {
                'metricas': {'metricas_gerais': {'maturidade_media': 0}},
                'cfd': {'wip': 0, 'throughput': 0},
                'alocacao': {'media_pessoas_squad': 0, 'composicao_squads': {}},
                'insights': ["‚ö†Ô∏è **Dados Insuficientes**: N√£o foi poss√≠vel gerar insights detalhados."]
            }
              
        # Calcular m√©tricas b√°sicas de maturidade
        maturidade_media = df['Maturidade'].mean() if 'Maturidade' in df.columns else 0
        maturidade_std = df['Maturidade'].std() if 'Maturidade' in df.columns else 0
        
        # Essas m√©tricas sempre funcionar√£o mesmo sem o CFD completo
        metricas_calculadas = {
            'metricas_gerais': {
                'maturidade_media': maturidade_media,
                'maturidade_std': maturidade_std,
            }
        }
        
        # Tentamos calcular m√©tricas √°geis se poss√≠vel
        try:
            metricas_ageis = analisar_metricas_ageis(df, tribo)
            if metricas_ageis:
                metricas_calculadas.update(metricas_ageis)
        except Exception as e:
            logging.warning(f"N√£o foi poss√≠vel calcular m√©tricas √°geis detalhadas: {str(e)}")
        
        # Preparar dados para CFD
        colunas_cfd = {}
        for coluna_necessaria in ['data', 'status', 'lead_time']:
            for coluna_real in df.columns:
                if coluna_necessaria.lower() in coluna_real.lower():
                    colunas_cfd[coluna_necessaria] = coluna_real
                    break
        
        # Calcular CFD - mesmo que falhe, continuamos com o que temos
        try:
            cfd = analisar_cfd(df, colunas_cfd)
        except Exception as e:
            logging.warning(f"Erro ao analisar CFD: {str(e)}")
            cfd = {'wip': 0, 'throughput': 0, 'avg_lead_time': None}
        
        # Analisar aloca√ß√£o
        alocacao = analisar_alocacao(analises.get('alocacao', pd.DataFrame()), tribo)
        
        # Gerar insights com os dados dispon√≠veis
        try:
            insights = gerar_insights_detalhados(metricas_calculadas, alocacao, cfd)
        except Exception as e:
            logging.warning(f"Erro ao gerar insights: {str(e)}")
            insights = ["‚ö†Ô∏è **Insights Limitados**: Dados insuficientes para an√°lise completa."]
        
        return {
            'metricas': metricas_calculadas,
            'cfd': cfd,
            'alocacao': alocacao,
            'insights': insights
        }
        
    except Exception as e:
        logging.error(f"Erro ao extrair m√©tricas: {str(e)}")
        raise

def verificar_dados(df: pd.DataFrame) -> bool:
    colunas = mapear_colunas_ageis(df)
    missing = [col for col, nome in colunas.items() if nome not in df.columns]
    if missing:
        logging.warning(f"Colunas ausentes: {missing}")
    return len(missing) == 0

def testar_mapeamento(df: pd.DataFrame) -> Dict[str, Any]:
    """Testa o mapeamento de colunas e retorna diagn√≥stico"""
    try:
        # Teste de mapeamento de colunas
        colunas = mapear_colunas_ageis(df)
        
        # Valida√ß√£o do resultado
        if not colunas:
            return {
                'status': 'error',
                'mensagem': 'Nenhuma coluna mapeada',
                'colunas_encontradas': []
            }
            
        # Verificar colunas cr√≠ticas
        colunas_criticas = ['data', 'status', 'tribo', 'squad']
        missing = [col for col in colunas_criticas if col not in colunas]
        
        if missing:
            return {
                'status': 'warning',
                'mensagem': f'Colunas cr√≠ticas ausentes: {", ".join(missing)}',
                'colunas_encontradas': list(colunas.keys())
            }
            
        return {
            'status': 'success',
            'mensagem': 'Mapeamento realizado com sucesso',
            'colunas_encontradas': list(colunas.keys()),
            'mapeamento': colunas
        }
        
    except Exception as e:
        logging.error(f"Erro ao testar mapeamento: {str(e)}")
        return {
            'status': 'error',
            'mensagem': f'Erro ao testar mapeamento: {str(e)}',
            'colunas_encontradas': []
        }

def executar_pipeline():
    """Executa o pipeline completo de an√°lise"""
    try:
        # Carregar dados
        dados = carregar_dados()
        
        # Log detalhado dos dados carregados
        for nome, df in dados.items():
            if df is None or df.empty:
                logging.warning(f"DataFrame {nome} est√° vazio")
            else:
                logging.info(f"DataFrame {nome} carregado com {len(df)} registros e colunas: {df.columns.tolist()}")
        
        # Valida√ß√£o cr√≠tica - apenas maturidade e aloca√ß√£o s√£o essenciais
        if dados['maturidade'].empty:
            raise ValueError("Dados de maturidade n√£o encontrados ou vazios")
            
        if dados['alocacao'].empty:
            raise ValueError("Dados de aloca√ß√£o n√£o encontrados ou vazios")
            
        # Dados executivos s√£o opcionais
        if dados['executivo'] is None or dados['executivo'].empty:
            logging.warning("Dados executivos n√£o encontrados ou vazios - continuando sem eles")
            dados['executivo'] = pd.DataFrame()  # DataFrame vazio para evitar erros
        
        # Log da quantidade de registros
        logging.info("Registros carregados:")
        logging.info(f"- Maturidade: {len(dados['maturidade'])} registros")
        logging.info(f"- Aloca√ß√£o: {len(dados['alocacao'])} registros")
        if not dados['executivo'].empty:
            logging.info(f"- Executivo: {len(dados['executivo'])} registros")
        
        # Testar mapeamento de colunas
        mapeamento_mat = testar_mapeamento(dados['maturidade'])
        mapeamento_aloc = testar_mapeamento(dados['alocacao'])
        
        if mapeamento_mat['status'] == 'error':
            raise ValueError(f"Erro no mapeamento de colunas de maturidade: {mapeamento_mat['mensagem']}")
            
        if mapeamento_aloc['status'] == 'error':
            raise ValueError(f"Erro no mapeamento de colunas de aloca√ß√£o: {mapeamento_aloc['mensagem']}")
        
        # Log do mapeamento
        logging.info("Mapeamento de colunas:")
        logging.info(f"- Maturidade: {mapeamento_mat['colunas_encontradas']}")
        logging.info(f"- Aloca√ß√£o: {mapeamento_aloc['colunas_encontradas']}")
        
        # Cruzar dados
        dados_cruzados = cruzar_dados(dados)
        logging.info(f"Dados cruzados: {len(dados_cruzados)} registros")
        
        # Extrair m√©tricas
        logging.info("Extraindo m√©tricas...")
        analises = {
            'dados_cruzados': dados_cruzados,
            'maturidade': dados['maturidade'],
            'alocacao': dados['alocacao']
        }
        
        metricas = extrair_metricas_ageis(analises)
        logging.info("M√©tricas extra√≠das com sucesso")
        
        return {
            "status": "success",
            "dados": analises,
            "metricas": metricas,
            "mensagem": f"Pipeline executado com sucesso. Registros processados: {len(dados_cruzados)}"
        }
        
    except FileNotFoundError as e:
        logging.error(f"Erro ao carregar dados: {str(e)}")
        return {
            "status": "error",
            "dados": None,
            "mensagem": f"Arquivo n√£o encontrado. Verifique se os arquivos Excel est√£o na pasta 'dados': {str(e)}"
        }
    except ValueError as e:
        logging.error(f"Erro de valida√ß√£o: {str(e)}")
        return {
            "status": "error",
            "dados": None,
            "mensagem": f"Erro nos dados: {str(e)}"
        }
    except Exception as e:
        logging.error(f"Erro no pipeline: {str(e)}")
        traceback.print_exc()
        return {
            "status": "error",
            "dados": None,
            "mensagem": f"Erro inesperado: {str(e)}"
        }

# Remova todo o c√≥digo abaixo desta linha
# N√£o execute nada automaticamente!
# O insights.py deve importar executar_pipeline e chat_ia_loop para orquestrar o fluxo.

def identificar_gargalos(cfd: pd.DataFrame) -> List[str]:
    """Identifica gargalos no fluxo de trabalho baseado no CFD"""
    gargalos = []
    
    if cfd.empty:
        logging.warning("CFD vazio - n√£o √© poss√≠vel identificar gargalos")
        return gargalos
        
    try:
        # Calcula a diferen√ßa entre status consecutivos
        for col in cfd.columns[:-1]:
            next_col = cfd.columns[cfd.columns.get_loc(col) + 1]
            diferenca = cfd[next_col] - cfd[col]
            
            # Se a diferen√ßa m√©dia √© maior que um threshold
            if diferenca.mean() > diferenca.std() * 2:
                gargalos.append(f"Gargalo identificado entre {col} e {next_col}")
                
        return gargalos
        
    except Exception as e:
        logging.error(f"Erro ao identificar gargalos: {str(e)}")
        return gargalos

def testar_analise_tribo(tribo: str):
    """Fun√ß√£o de teste para validar an√°lise de tribo"""
    try:
        # Carregar dados
        dados = carregar_dados()
        if not dados:
            raise ValueError("N√£o foi poss√≠vel carregar os dados")
            
        # Filtrar dados da tribo
        df_tribo = dados['maturidade'][dados['maturidade']['Tribo'] == tribo]
        if df_tribo.empty:
            raise ValueError(f"Tribo '{tribo}' n√£o encontrada nos dados")
            
        # Executar an√°lises
        metricas = analisar_metricas_ageis(df_tribo)
        if not metricas:
            raise ValueError("Falha ao calcular m√©tricas √°geis")
            
        return {
            "status": "success",
            "tribo": tribo,
            "metricas": metricas
        }
        
    except Exception as e:
        logging.error(f"Erro no teste de an√°lise da tribo {tribo}: {str(e)}")
        return {
            "status": "error",
            "tribo": tribo,
            "erro": str(e)
        }

def mapear_estrutura_org(analises: Dict[str, Any]) -> Dict:
    """Mapeia a estrutura organizacional a partir das an√°lises"""
    try:
        estrutura = {
            'tribos': {},
            'total_squads': 0,
            'total_pessoas': 0,
            'papeis_total': {},
            'roles': {},  # Adicionado para mapear pap√©is e suas descri√ß√µes
            'maturidades': {}  # Adicionado para armazenar dados de maturidade
        }
        
        # Verificar se todos os dados necess√°rios est√£o presentes
        if not analises:
            logging.warning("Dados de an√°lise vazios ou inexistentes")
            return estrutura
            
        # Extrair dados de aloca√ß√£o e maturidade
        df_alocacao = analises.get('alocacao', pd.DataFrame())
        df_maturidade = analises.get('maturidade', pd.DataFrame())
        
        if df_alocacao is None or df_alocacao.empty:
            logging.warning("DataFrame de aloca√ß√£o vazio")
            # Dados de exemplo para estrutura - permitir funcionamento mesmo sem dados
            estrutura['tribos']['Exemplo'] = {
                'squads': ['Squad 1', 'Squad 2'],
                'total_pessoas': 10,
                'papeis': {'Desenvolvedor': 5, 'PO': 1, 'QA': 2, 'Tech Lead': 2}
            }
            estrutura['total_squads'] = 2
            estrutura['total_pessoas'] = 10
            estrutura['papeis_total'] = {'Desenvolvedor': 5, 'PO': 1, 'QA': 2, 'Tech Lead': 2}
            estrutura['roles'] = {
                'Desenvolvedor': 'Respons√°vel pelo desenvolvimento de c√≥digo',
                'PO': 'Product Owner, respons√°vel pelo backlog do produto',
                'QA': 'Quality Assurance, respons√°vel pela qualidade',
                'Tech Lead': 'L√≠der t√©cnico do squad'
            }
            return estrutura
            
        # Verificar as colunas necess√°rias - buscar por varia√ß√µes nos nomes das colunas
        colunas_mapeadas = {}
        colunas_necessarias_base = ['tribe', 'squad', 'person', 'role']
        colunas_alternativas = {
            'tribe': ['tribe', 'tribo', 'Tribo', 'Tribe'],
            'squad': ['squad', 'Squad', 'equipe', 'Equipe', 'time', 'Time'],
            'person': ['person', 'pessoa', 'Pessoa', 'nome', 'Nome', 'colaborador', 'Colaborador'],
            'role': ['role', 'papel', 'Papel', 'cargo', 'Cargo', 'funcao', 'Funcao', 'fun√ß√£o', 'Fun√ß√£o']
        }
        
        # Tentar encontrar colunas alternativas
        for col_base, alternativas in colunas_alternativas.items():
            for alt in alternativas:
                if alt in df_alocacao.columns:
                    colunas_mapeadas[col_base] = alt
                    break
            
        # Verificar colunas faltantes ap√≥s tentativa de mapeamento
        colunas_faltantes = [col for col in colunas_necessarias_base if col not in colunas_mapeadas]
        
        if colunas_faltantes:
            logging.warning(f"Colunas necess√°rias ausentes mesmo ap√≥s mapeamento: {colunas_faltantes}")
            # Adicionar colunas vazias para evitar erros
            for col in colunas_faltantes:
                df_alocacao[col] = 'Indefinido'
                colunas_mapeadas[col] = col
        
        # Usar colunas mapeadas        
        tribe_col = colunas_mapeadas.get('tribe', 'tribe')
        squad_col = colunas_mapeadas.get('squad', 'squad') 
        person_col = colunas_mapeadas.get('person', 'person')
        role_col = colunas_mapeadas.get('role', 'role')
        
        # Normalizar valores para evitar duplica√ß√µes por diferen√ßas de capitaliza√ß√£o/acentos
        df_alocacao['tribe_norm'] = df_alocacao[tribe_col].apply(lambda x: unidecode(str(x).lower()))
        df_alocacao['squad_norm'] = df_alocacao[squad_col].apply(lambda x: unidecode(str(x).lower()))
                
        # Mapear tribos e squads
        for tribo, tribo_norm in zip(df_alocacao[tribe_col].unique(), df_alocacao['tribe_norm'].unique()):
            if pd.isna(tribo) or not tribo:
                continue
                
            df_tribo = df_alocacao[df_alocacao['tribe_norm'] == tribo_norm]
            
            # Usar nome original, n√£o normalizado
            estrutura['tribos'][tribo] = {
                'squads': list(df_tribo[squad_col].unique()),
                'total_pessoas': len(df_tribo[person_col].unique()),
                'papeis': dict(Counter(df_tribo[role_col])),
                'nome_normalizado': tribo_norm  # Adicionar nome normalizado para busca
            }
        
        # Calcular totais
        estrutura['total_squads'] = len(df_alocacao[squad_col].unique())
        estrutura['total_pessoas'] = len(df_alocacao[person_col].unique())
        estrutura['papeis_total'] = dict(Counter(df_alocacao[role_col]))
        
        # Adicionar descri√ß√µes de pap√©is
        papeis_descricoes = {
            'desenvolvedor': 'Respons√°vel pelo desenvolvimento de c√≥digo',
            'po': 'Product Owner, respons√°vel pelo backlog do produto',
            'qa': 'Quality Assurance, respons√°vel pela qualidade',
            'tech lead': 'L√≠der t√©cnico do squad',
            'scrum master': 'Facilitador do processo √°gil',
            'agile master': 'Facilitador do processo √°gil',
            'ux': 'User Experience, respons√°vel pela experi√™ncia do usu√°rio',
            'ui': 'User Interface, respons√°vel pela interface do usu√°rio',
            'analista': 'Analista de neg√≥cios ou sistemas',
            'arquiteto': 'Arquiteto de software ou solu√ß√µes',
            'gerente': 'Gerente de produto ou projeto',
            'devops': 'Respons√°vel pela integra√ß√£o entre desenvolvimento e opera√ß√µes',
            'sre': 'Site Reliability Engineer, respons√°vel pela confiabilidade',
            'data scientist': 'Cientista de dados, respons√°vel por an√°lises avan√ßadas',
            'data engineer': 'Engenheiro de dados, respons√°vel por pipelines de dados'
        }
          # Mapear pap√©is encontrados para as descri√ß√µes
        for papel in estrutura['papeis_total'].keys():
            papel_norm = unidecode(str(papel).lower())
            for desc_key, desc in papeis_descricoes.items():
                if desc_key in papel_norm or papel_norm in desc_key:
                    estrutura['roles'][papel] = desc
                    break
            else:
                estrutura['roles'][papel] = 'Fun√ß√£o na organiza√ß√£o'
        
        # Processar dados de maturidade se dispon√≠veis
        if df_maturidade is not None and not df_maturidade.empty:
            if 'Tribo' in df_maturidade.columns and 'Maturidade' in df_maturidade.columns:
                logging.info("Processando dados de maturidade das tribos")
                
                # Criar mapeamento de tribos para facilitar busca por nome normalizado
                nomes_tribos_normalizados = {}
                for tribo in estrutura['tribos'].keys():
                    tribo_norm = normalizar_texto(tribo)
                    nomes_tribos_normalizados[tribo_norm] = tribo
                
                # Adicionar maturidade para cada tribo
                for _, row in df_maturidade.iterrows():
                    tribo_nome = row['Tribo']
                    maturidade = row['Maturidade']
                    
                    if pd.notna(tribo_nome) and pd.notna(maturidade):
                        # Tentar encontrar tribo existente usando normaliza√ß√£o
                        tribo_norm = normalizar_texto(tribo_nome)
                        
                        # Primeiro, verificar se a tribo existe diretamente
                        if tribo_nome in estrutura['tribos']:
                            estrutura['tribos'][tribo_nome]['maturidade'] = float(maturidade)
                            estrutura['maturidades'][tribo_nome] = float(maturidade)
                        
                        # Caso contr√°rio, tentar encontrar por nome normalizado
                        elif tribo_norm in nomes_tribos_normalizados:
                            tribo_key = nomes_tribos_normalizados[tribo_norm]
                            estrutura['tribos'][tribo_key]['maturidade'] = float(maturidade)
                            estrutura['maturidades'][tribo_key] = float(maturidade)
                        
                        # Se ainda n√£o encontrou, criar um novo registro para a tribo
                        else:
                            estrutura['maturidades'][tribo_nome] = float(maturidade)
                            # Se a tribo n√£o existe na estrutura, adicionar com informa√ß√µes m√≠nimas
                            if tribo_nome not in estrutura['tribos']:
                                estrutura['tribos'][tribo_nome] = {
                                    'squads': [],
                                    'total_pessoas': 0,
                                    'papeis': {},
                                    'maturidade': float(maturidade),
                                    'nome_normalizado': tribo_norm
                                }
                
                # Log das maturidades carregadas                logging.info(f"Maturidades carregadas: {estrutura['maturidades']}")
        
        return estrutura
        
    except Exception as e:
        logging.error(f"Erro ao mapear estrutura organizacional: {str(e)}")
        tb_str = traceback.format_exc()
        logging.error(f"Traceback: {tb_str}")
        return {
            'tribos': {},
            'total_squads': 0,
            'total_pessoas': 0,
            'papeis_total': {},
            'roles': {},
            'maturidades': {}
        }

def identificar_entidade_consulta(query: str, estrutura: Dict) -> Tuple[str, str]:
    """Identifica o tipo de entidade (tribo/squad) e seu nome na consulta"""
    # Verificar se query √© v√°lida
    if not query or not isinstance(query, str):
        return None, None
        
    # Verificar se estrutura est√° definida corretamente
    if not estrutura or not isinstance(estrutura, dict) or 'tribos' not in estrutura:
        logging.warning("Estrutura inv√°lida ou tribos n√£o definidas em identificar_entidade_consulta")
        return None, None
        
    # Normalizar a consulta para busca insens√≠vel ao caso e sem acentos
    query_norm = normalizar_texto(query)
    
    # Verificar se a query menciona explicitamente "tribo" ou "squad"
    tipo_mencionado = None
    if "tribo" in query_norm:
        tipo_mencionado = "tribo"
    elif "squad" in query_norm or "equipe" in query_norm or "time" in query_norm:
        tipo_mencionado = "squad"
    
    # Procurar por tribos - primeiro tentamos o nome exato
    for tribo in estrutura['tribos'].keys():
        if tribo.lower() in query.lower():
            logging.info(f"Entidade identificada como tribo: {tribo}")
            return 'tribo', tribo
    
    # Se n√£o achar com nome exato, tentar com nome normalizado
    for tribo, info in estrutura['tribos'].items():
        tribo_norm = normalizar_texto(tribo)
        if tribo_norm in query_norm:
            logging.info(f"Entidade identificada como tribo ap√≥s normaliza√ß√£o: {tribo}")
            return 'tribo', tribo
        
        # Verificar nome normalizado armazenado na estrutura (se existir)
        if isinstance(info, dict) and 'nome_normalizado' in info and info['nome_normalizado'] in query_norm:
            logging.info(f"Entidade identificada como tribo usando nome_normalizado: {tribo}")
            return 'tribo', tribo
              # Procurar por squads - primeiro tentamos o nome exato
    for tribo_info in estrutura['tribos'].values():
        # Verificar se 'squads' existe e √© uma lista
        if not isinstance(tribo_info, dict) or 'squads' not in tribo_info or not isinstance(tribo_info['squads'], list):
            continue
            
        for squad in tribo_info['squads']:
            if squad and isinstance(squad, str) and squad.lower() in query.lower():
                logging.info(f"Entidade identificada como squad: {squad}")
                return 'squad', squad
    
    # Se n√£o achar com nome exato, tentar com nome normalizado
    for tribo_info in estrutura['tribos'].values():
        # Verificar se 'squads' existe e √© uma lista
        if not isinstance(tribo_info, dict) or 'squads' not in tribo_info or not isinstance(tribo_info['squads'], list):
            continue
            
        for squad in tribo_info['squads']:
            if squad and isinstance(squad, str):
                squad_norm = normalizar_texto(squad)
                if squad_norm in query_norm:
                    logging.info(f"Entidade identificada como squad ap√≥s normaliza√ß√£o: {squad}")
                    return 'squad', squad
      # Se mencionou explicitamente "tribo" ou "squad", mas n√£o encontramos o nome
    if tipo_mencionado:
        # Tentar extrair o nome da entidade do contexto da frase
        words = query_norm.split()
        for i, word in enumerate(words):
            if word == tipo_mencionado and i < len(words) - 1:
                # Verificar palavra seguinte e subsequentes como poss√≠vel nome
                nome_candidato = words[i+1]
                # Se uma palavra ap√≥s "tribo" tem mais de 3 caracteres, pode ser um nome
                if len(nome_candidato) > 3 and nome_candidato not in ["da", "de", "do", "dos", "das"]:
                    # Verificar se √© um nome parcial de alguma tribo/squad
                    if tipo_mencionado == "tribo":
                        for tribo in estrutura['tribos'].keys():
                            tribo_norm = normalizar_texto(tribo)
                            if nome_candidato in tribo_norm:
                                logging.info(f"Entidade tribo identificada por palavra-chave: {tribo}")
                                return 'tribo', tribo
                    elif tipo_mencionado == "squad":
                        for tribo_info in estrutura['tribos'].values():
                            if not isinstance(tribo_info, dict) or 'squads' not in tribo_info:
                                continue
                            for squad in tribo_info['squads']:
                                if not squad or not isinstance(squad, str):
                                    continue
                                squad_norm = normalizar_texto(squad)
                                if nome_candidato in squad_norm:
                                    logging.info(f"Entidade squad identificada por palavra-chave: {squad}")
                                    return 'squad', squad
    
    # Busca por palavras espec√≠ficas nos nomes das tribos (para casos como "vendas", "benef√≠cios", etc.)
    for tribo in estrutura['tribos'].keys():
        tribo_words = set(normalizar_texto(tribo).split())
        query_words = set(query_norm.split())
        common_words = tribo_words.intersection(query_words)
        # Se h√° pelo menos uma palavra em comum que n√£o seja uma stopword
        if common_words and not all(word in ["de", "da", "do", "para", "em", "com", "por"] for word in common_words):
            logging.info(f"Entidade tribo identificada por palavras em comum: {tribo}")
            return 'tribo', tribo
    
    # Se n√£o encontrou nada espec√≠fico, veja se √© uma consulta geral sobre tribo ou squad
    if "tribo" in query_norm and not tipo_mencionado:
        logging.info("Consulta geral sobre tribos identificada")
        return 'tribo_geral', 'tribo'
    elif ("squad" in query_norm or "equipe" in query_norm or "time" in query_norm) and not tipo_mencionado:
        logging.info("Consulta geral sobre squads identificada")
        return 'squad_geral', 'squad'

    logging.warning(f"N√£o foi poss√≠vel identificar entidade na consulta: '{query}'")        
    return None, None

def preparar_dados_consulta(entidade: str, nome: str, estrutura: Dict, analises: Dict) -> Dict:
    """Prepara dados relevantes baseado na entidade consultada"""
    if not entidade or not nome:
        return analises
        
    dados_filtrados = {
        'estrutura_organizacional': estrutura  # Sempre incluir a estrutura organizacional para contexto
    }
    
    # Adicionar m√©tricas e insights dispon√≠veis
    for key in ['metricas', 'insights']:
        if key in analises:
            dados_filtrados[key] = analises[key]
    
    # Normalizar o nome para compara√ß√£o 
    nome_norm = unidecode(nome.lower()) if nome else ""
    
    if entidade == 'tribo':
        # Filtrar dados da tribo
        for key, df in analises.items():
            if not isinstance(df, pd.DataFrame) or df.empty:
                dados_filtrados[key] = df
                continue
                
            # Para cada DataFrame, encontrar a coluna correta para filtragem
            if 'tribe' in df.columns:
                # Primeiro tenta correspond√™ncia exata
                filtered_df = df[df['tribe'] == nome]
                # Se n√£o encontrar, tenta normalizada
                if filtered_df.empty:
                    filtered_df = df[df['tribe'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                dados_filtrados[key] = filtered_df
            elif 'Tribo' in df.columns:
                # Criamos uma coluna normalizada tempor√°ria para compara√ß√£o
                df['temp_tribo_norm'] = df['Tribo'].apply(normalizar_texto)
                # Filtramos usando a coluna normalizada
                filtered_df = df[df['temp_tribo_norm'] == nome_norm]
                # Removemos a coluna tempor√°ria
                df = df.drop('temp_tribo_norm', axis=1)
                dados_filtrados[key] = filtered_df
            else:
                # Busca por outras colunas que possam conter o nome da tribo
                found = False
                for col in df.columns:
                    if 'tribo' in col.lower() or 'tribe' in col.lower():
                        # Primeiro tenta correspond√™ncia exata
                        filtered_df = df[df[col] == nome]
                        # Se n√£o encontrar, tenta normalizada
                        if filtered_df.empty:
                            filtered_df = df[df[col].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                        
                        if not filtered_df.empty:
                            dados_filtrados[key] = filtered_df
                            found = True
                            break
                
                if not found:
                    dados_filtrados[key] = df
                    
    elif entidade == 'squad':
        # Filtrar dados do squad
        for key, df in analises.items():
            if not isinstance(df, pd.DataFrame) or df.empty:
                dados_filtrados[key] = df
                continue
                
            # Para cada DataFrame, encontrar a coluna correta para filtragem
            if 'squad' in df.columns:
                # Primeiro tenta correspond√™ncia exata
                filtered_df = df[df['squad'] == nome]
                # Se n√£o encontrar, tenta normalizada
                if filtered_df.empty:
                    filtered_df = df[df['squad'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                dados_filtrados[key] = filtered_df
            elif 'Squad' in df.columns:
                # Primeiro tenta correspond√™ncia exata
                filtered_df = df[df['Squad'] == nome]
                # Se n√£o encontrar, tenta normalizada
                if filtered_df.empty:
                    filtered_df = df[df['Squad'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                dados_filtrados[key] = filtered_df
            else:
                # Busca por outras colunas que possam conter o nome do squad
                found = False
                for col in df.columns:
                    if 'squad' in col.lower() or 'equipe' in col.lower() or 'time' in col.lower():
                        # Primeiro tenta correspond√™ncia exata
                        filtered_df = df[df[col] == nome]
                        # Se n√£o encontrar, tenta normalizada
                        if filtered_df.empty:
                            filtered_df = df[df[col].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                        
                        if not filtered_df.empty:
                            dados_filtrados[key] = filtered_df
                            found = True
                            break
                
                if not found:
                    dados_filtrados[key] = df
    else:
        # Se n√£o for tribo nem squad, retornar os dados originais
        return analises
    
    # Log de diagn√≥stico
    for key, df in dados_filtrados.items():
        if isinstance(df, pd.DataFrame):
            logging.debug(f"Dados filtrados para {entidade} '{nome}' - {key}: {len(df)} registros")
            
    return dados_filtrados

def gerar_resposta_contextualizada(query: str, entidade: str, dados: Dict, client: OpenAI) -> str:
    """Gera resposta contextualizada usando OpenAI"""
    try:
        # Preparar contexto com verifica√ß√£o robusta
        contexto = []
        tipo_entidade = None
        
        # Verificar se dados √© v√°lido
        if not dados or not isinstance(dados, dict):
            return "N√£o foi poss√≠vel processar os dados para gerar uma resposta contextualizada."
        
        # Tentar obter m√©tricas previamente calculadas - esta √© uma fonte rica de informa√ß√µes
        metricas = dados.get('metricas', {})
        if isinstance(metricas, dict) and metricas:
            contexto.append("=== M√©tricas Dispon√≠veis ===")
        
        # Dados de insights (preexistentes)
        insights = metricas.get('insights', [])
        if isinstance(insights, list) and insights:
            contexto.append("--- Insights Principais ---")
            for insight in insights[:5]:  # Limitamos a 5 insights para n√£o sobrecarregar
                contexto.append(insight)
        
        # Verificar se a entidade existe nos dados e qual o tipo
        # Tentar determinar se a entidade √© uma tribo ou um squad
        mat = dados.get('maturidade', pd.DataFrame())
        aloc = dados.get('alocacao', pd.DataFrame())
        
        # Normalizar nome da entidade para busca case-insensitive e sem acentos
        nome_norm = unidecode(entidade.lower()) if entidade else ""
        
        # Verificar se √© uma tribo
        is_tribo = False
        if isinstance(mat, pd.DataFrame) and not mat.empty and 'Tribo' in mat.columns:
            tribos = mat['Tribo'].apply(lambda x: unidecode(str(x).lower()))
            if nome_norm in tribos.values:
                is_tribo = True
                tipo_entidade = 'tribo'
        
        if not is_tribo and isinstance(aloc, pd.DataFrame) and not aloc.empty:
            if 'tribe' in aloc.columns:
                tribos = aloc['tribe'].apply(lambda x: unidecode(str(x).lower()))
                if nome_norm in tribos.values:
                    is_tribo = True
                    tipo_entidade = 'tribo'
            elif 'Tribo' in aloc.columns:
                tribos = aloc['Tribo'].apply(lambda x: unidecode(str(x).lower()))
                if nome_norm in tribos.values:
                    is_tribo = True
                    tipo_entidade = 'tribo'
        
        # Verificar se √© um squad
        is_squad = False
        if isinstance(aloc, pd.DataFrame) and not aloc.empty:
            if 'squad' in aloc.columns:
                squads = aloc['squad'].apply(lambda x: unidecode(str(x).lower()))
                if nome_norm in squads.values:
                    is_squad = True
                    tipo_entidade = 'squad'
            elif 'Squad' in aloc.columns:
                squads = aloc['Squad'].apply(lambda x: unidecode(str(x).lower()))
                if nome_norm in squads.values:
                    is_squad = True
                    tipo_entidade = 'squad'
          # Se n√£o foi poss√≠vel determinar o tipo, verificar se o nome da entidade √© 'tribo' ou 'squad'
        if not tipo_entidade and entidade is not None:
            if entidade.lower() == 'tribo':
                tipo_entidade = 'tribo_geral'
            elif entidade.lower() == 'squad':
                tipo_entidade = 'squad_geral'
        elif not tipo_entidade:
            # Caso a entidade seja None, definir como an√°lise geral
            tipo_entidade = 'analise_geral'
        
        # Processar dados de acordo com o tipo de entidade
        if tipo_entidade == 'tribo' or is_tribo:
            contexto.append(f"=== An√°lise da Tribo: {entidade} ===")
            
            # Carregar dados de maturidade com verifica√ß√µes de seguran√ßa
            if isinstance(mat, pd.DataFrame) and not mat.empty:
                # Tentar diferentes formatos de coluna para a tribo
                if 'Tribo' in mat.columns:
                    # Filtrar usando normaliza√ß√£o
                    mat_filtered = mat[mat['Tribo'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                    if not mat_filtered.empty and 'Maturidade' in mat_filtered.columns:
                        contexto.append(f"Maturidade m√©dia: {mat_filtered['Maturidade'].mean():.1f}")
                    elif 'Maturidade' in mat.columns:  # Fallback para todos os dados
                        contexto.append(f"Maturidade m√©dia geral: {mat['Maturidade'].mean():.1f}")
                        contexto.append("Nota: Dados espec√≠ficos da tribo n√£o encontrados, usando m√©dia geral.")
                else:
                    # Tentar outras varia√ß√µes do nome da coluna
                    for col in mat.columns:
                        if 'tribo' in col.lower() or 'tribe' in col.lower():
                            mat_filtered = mat[mat[col].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                            if not mat_filtered.empty and 'Maturidade' in mat_filtered.columns:
                                contexto.append(f"Maturidade m√©dia: {mat_filtered['Maturidade'].mean():.1f}")
                                break
                    else:
                        if 'Maturidade' in mat.columns:  # Fallback para todos os dados
                            contexto.append(f"Maturidade m√©dia geral: {mat['Maturidade'].mean():.1f}")
                            contexto.append("Nota: Dados espec√≠ficos da tribo n√£o encontrados, usando m√©dia geral.")
                
            # Carregar dados de aloca√ß√£o com verifica√ß√µes de seguran√ßa
            if isinstance(aloc, pd.DataFrame) and not aloc.empty:
                # Primeiramente tenta filtrar por tribo se poss√≠vel
                aloc_filtered = aloc
                if 'tribe' in aloc.columns:
                    aloc_filtered = aloc[aloc['tribe'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                elif 'Tribo' in aloc.columns:
                    aloc_filtered = aloc[aloc['Tribo'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                
                # Se o filtro resultou em dados vazios, usar dados completos com aviso
                if aloc_filtered.empty:
                    aloc_filtered = aloc
                    contexto.append("Nota: Dados espec√≠ficos da tribo n√£o encontrados, usando dados gerais.")
                
                # Extrair informa√ß√µes de pessoas e pap√©is
                if 'person' in aloc_filtered.columns:
                    contexto.append(f"Pessoas alocadas: {len(aloc_filtered['person'].unique())}")
                
                if 'squad' in aloc_filtered.columns:    
                    contexto.append(f"Squads: {len(aloc_filtered['squad'].unique())}")
                    
                if 'role' in aloc_filtered.columns:
                    contexto.append(f"Pap√©is: {dict(Counter(aloc_filtered['role']))}")
                    
                # Adicionar informa√ß√µes sobre composi√ß√£o de squads
                if 'squad' in aloc_filtered.columns and 'person' in aloc_filtered.columns:
                    pessoas_por_squad = aloc_filtered.groupby('squad')['person'].nunique()
                    if not pessoas_por_squad.empty:
                        contexto.append(f"Pessoas por squad (m√©dia): {pessoas_por_squad.mean():.1f}")
                        contexto.append(f"Pessoas por squad (m√≠nimo): {pessoas_por_squad.min()}")
                        contexto.append(f"Pessoas por squad (m√°ximo): {pessoas_por_squad.max()}")
                
            # Adicionar m√©tricas de CFD e outras m√©tricas √°geis se dispon√≠veis
            cfd = dados.get('metricas', {}).get('cfd', {})
            if isinstance(cfd, dict):
                if 'throughput' in cfd:
                    contexto.append(f"Throughput: {cfd['throughput']:.2f} itens/m√™s")
                if 'wip' in cfd:
                    contexto.append(f"WIP atual: {cfd['wip']}")
                if 'avg_lead_time' in cfd and cfd['avg_lead_time'] is not None:
                    contexto.append(f"Lead time m√©dio: {cfd['avg_lead_time']:.2f} dias")
                
        # Caso do Squad
        elif tipo_entidade == 'squad' or is_squad:
            contexto.append(f"=== An√°lise do Squad: {entidade} ===")
            
            if isinstance(aloc, pd.DataFrame) and not aloc.empty:
                # Filtrar por squad
                aloc_filtered = aloc
                if 'squad' in aloc.columns:
                    aloc_filtered = aloc[aloc['squad'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                elif 'Squad' in aloc.columns:
                    aloc_filtered = aloc[aloc['Squad'].apply(lambda x: unidecode(str(x).lower()) == nome_norm)]
                
                # Se o filtro resultou em dados vazios, usar aviso
                if aloc_filtered.empty:
                    contexto.append(f"Nota: N√£o foram encontrados dados para o squad '{entidade}'.")
                else:
                    if 'person' in aloc_filtered.columns:
                        contexto.append(f"Pessoas no squad: {len(aloc_filtered['person'].unique())}")
                    
                    if 'role' in aloc_filtered.columns:
                        contexto.append(f"Pap√©is: {dict(Counter(aloc_filtered['role']))}")
                        
                    # Se tivermos a tribo do squad, adicionar essa informa√ß√£o
                    if 'tribe' in aloc_filtered.columns:
                        tribos = aloc_filtered['tribe'].unique()
                        if len(tribos) > 0:
                            contexto.append(f"Pertence √† tribo: {tribos[0]}")
        
        # Caso em que a entidade n√£o foi identificada como tribo ou squad
        elif tipo_entidade == 'tribo_geral':
            contexto.append("=== Vis√£o Geral das Tribos ===")
            
            # Listar todas as tribos dispon√≠veis
            tribos_disponiveis = []
            if isinstance(mat, pd.DataFrame) and not mat.empty:
                if 'Tribo' in mat.columns:
                    tribos_disponiveis.extend(mat['Tribo'].unique().tolist())
                    
            if isinstance(aloc, pd.DataFrame) and not aloc.empty:
                if 'tribe' in aloc.columns:
                    tribos_disponiveis.extend(aloc['tribe'].unique().tolist())
                elif 'Tribo' in aloc.columns:
                    tribos_disponiveis.extend(aloc['Tribo'].unique().tolist())
            
            tribos_disponiveis = list(set([t for t in tribos_disponiveis if t and pd.notna(t)]))
            if tribos_disponiveis:
                contexto.append(f"Tribos dispon√≠veis: {', '.join(tribos_disponiveis)}")
                contexto.append(f"Total de tribos: {len(tribos_disponiveis)}")
                
            # Estat√≠sticas gerais sobre tribos
            if isinstance(mat, pd.DataFrame) and not mat.empty and 'Maturidade' in mat.columns:
                contexto.append(f"Maturidade m√©dia geral: {mat['Maturidade'].mean():.1f}")
                contexto.append(f"Maturidade m√≠nima: {mat['Maturidade'].min():.1f}")
                contexto.append(f"Maturidade m√°xima: {mat['Maturidade'].max():.1f}")        # Caso da an√°lise geral de squads
        elif tipo_entidade == 'squad_geral':
            contexto.append("=== Vis√£o Geral dos Squads ===")
            
        # Caso de an√°lise geral quando n√£o foi poss√≠vel identificar a entidade
        elif tipo_entidade == 'analise_geral':
            contexto.append("=== An√°lise Geral da Organiza√ß√£o ===")
            
            # Listar todos os squads dispon√≠veis
            squads_disponiveis = []
            if isinstance(aloc, pd.DataFrame) and not aloc.empty:
                if 'squad' in aloc.columns:
                    squads_disponiveis.extend(aloc['squad'].unique().tolist())
                elif 'Squad' in aloc.columns:
                    squads_disponiveis.extend(aloc['Squad'].unique().tolist())
            
            squads_disponiveis = list(set([s for s in squads_disponiveis if s and pd.notna(s)]))
            if squads_disponiveis:
                # N√£o listar todos os squads se forem muitos, apenas mostrar a contagem
                if len(squads_disponiveis) > 10:
                    contexto.append(f"Total de squads: {len(squads_disponiveis)}")
                    contexto.append(f"Exemplos de squads: {', '.join(squads_disponiveis[:5])}")
                else:
                    contexto.append(f"Squads dispon√≠veis: {', '.join(squads_disponiveis)}")
                    contexto.append(f"Total de squads: {len(squads_disponiveis)}")
            
            # Estat√≠sticas gerais sobre squads
            if isinstance(aloc, pd.DataFrame) and not aloc.empty:
                if 'squad' in aloc.columns and 'person' in aloc.columns:
                    pessoas_por_squad = aloc.groupby('squad')['person'].nunique()
                    if not pessoas_por_squad.empty:
                        contexto.append(f"M√©dia de pessoas por squad: {pessoas_por_squad.mean():.1f}")
                        contexto.append(f"M√≠nimo de pessoas por squad: {pessoas_por_squad.min()}")
                        contexto.append(f"M√°ximo de pessoas por squad: {pessoas_por_squad.max()}")
                        
                if 'role' in aloc.columns:
                    papeis = Counter(aloc['role'])
                    contexto.append(f"Pap√©is mais comuns: {dict(papeis.most_common(5))}")
        
        # Buscar dados adicionais em outros dataframes que possam ter rela√ß√£o com a entidade
        # Analisar dados em outros dataframes (storylog, demandas, etc.)
        for key, df in dados.items():
            # Pular chaves que j√° processamos ou que n√£o s√£o dataframes
            if key in ['metricas', 'maturidade', 'alocacao', 'dados_cruzados'] or not isinstance(df, pd.DataFrame) or df.empty:
                continue
                
            # Verificar se h√° colunas relacionadas a tribo ou squad
            entity_columns = [col for col in df.columns if 'tribo' in col.lower() or 'tribe' in col.lower() or 'squad' in col.lower()]
            
            if entity_columns and nome_norm:
                for col in entity_columns:
                    filtered_df = df[df[col].apply(lambda x: unidecode(str(x).lower()) == nome_norm if pd.notna(x) else False)]
                    if not filtered_df.empty:
                        contexto.append(f"=== Dados adicionais de {key} ===")
                        contexto.append(f"Registros encontrados: {len(filtered_df)}")
                        
                        # Analisar colunas num√©ricas
                        num_cols = filtered_df.select_dtypes(include=['int64', 'float64']).columns
                        for num_col in num_cols[:3]:  # Limitar a 3 colunas num√©ricas
                            if filtered_df[num_col].notna().sum() > 0:
                                contexto.append(f"{num_col} (m√©dia): {filtered_df[num_col].mean():.2f}")
                        
                        # Analisar colunas de data
                        date_cols = [c for c in filtered_df.columns if 'data' in c.lower() or 'date' in c.lower()]
                        if date_cols:
                            for date_col in date_cols[:2]:  # Limitar a 2 colunas de data
                                if filtered_df[date_col].notna().sum() > 0:
                                    try:
                                        min_date = filtered_df[date_col].min()
                                        max_date = filtered_df[date_col].max()
                                        if pd.notna(min_date) and pd.notna(max_date):
                                            contexto.append(f"{date_col}: de {min_date} at√© {max_date}")
                                    except:
                                        pass
                                        
                        # Se h√° coluna de status, mostrar estat√≠sticas
                        status_cols = [c for c in filtered_df.columns if 'status' in c.lower() or 'situacao' in c.lower() or 'state' in c.lower()]
                        if status_cols:
                            for status_col in status_cols[:1]:  # Limitar a 1 coluna de status
                                status_counts = filtered_df[status_col].value_counts()
                                if not status_counts.empty:
                                    contexto.append(f"{status_col}: {dict(status_counts)}")
                        
                        # Adicionar m√©tricas relevantes calculadas
                        if key.lower() in ('demandas', 'stories', 'pbi', 'epicos'):
                            # Aqui podemos calcular m√©tricas espec√≠ficas para demandas
                            if 'status' in filtered_df.columns:
                                contexto.append(f"Distribui√ß√£o por status: {dict(filtered_df['status'].value_counts())}")
                            
                            # Contar itens por m√™s se houver data de cria√ß√£o
                            date_created_cols = [c for c in filtered_df.columns if 'cria' in c.lower() or 'create' in c.lower()]
                            if date_created_cols:
                                try:
                                    # Tentar an√°lise de tend√™ncia mensal
                                    filtered_df['month'] = pd.to_datetime(filtered_df[date_created_cols[0]]).dt.to_period('M')
                                    por_mes = filtered_df['month'].value_counts().sort_index()
                                    if len(por_mes) > 0:
                                        contexto.append(f"Tend√™ncia mensal (√∫ltimos 3 meses): {dict(por_mes[-3:])}")
                                except:
                                    pass
                        break
                        
        # Adicionar insights espec√≠ficos relacionados √† entidade
        if nome_norm:
            all_insights = metricas.get('insights', [])
            # Buscar por correspond√™ncias parciais no texto do insight
            entity_insights = [i for i in all_insights if unidecode(entidade.lower()) in unidecode(i.lower())]
            if entity_insights:
                contexto.append(f"--- Insights espec√≠ficos para {entidade} ---")
                for insight in entity_insights[:3]:  # Limitar a 3 insights espec√≠ficos
                    contexto.append(insight)
        
        # Se ainda n√£o houver contexto suficiente, tentar encontrar dados relacionados
        if len(contexto) < 5:
            # Procurar dados relacionados √† consulta
            dados_cruzados = dados.get('dados_cruzados', pd.DataFrame())
            if isinstance(dados_cruzados, pd.DataFrame) and not dados_cruzados.empty:
                contexto.append("=== Dados Relacionados ===")
                # Adicionar n√∫mero total de registros
                contexto.append(f"Total de registros: {len(dados_cruzados)}")
                # Adicionar informa√ß√µes sobre as colunas dispon√≠veis
                if dados_cruzados.columns.size > 0:
                    contexto.append(f"Colunas dispon√≠veis: {', '.join(dados_cruzados.columns[:10])}...")
                    
                    # Verificar se h√° colunas espec√≠ficas que podem ser relevantes
                    # Colunas de tempo/prazo
                    prazo_cols = [col for col in dados_cruzados.columns if 'prazo' in col.lower() or 'time' in col.lower() or 'lead' in col.lower()]
                    if prazo_cols:
                        for col in prazo_cols[:2]:
                            if dados_cruzados[col].dtype in ['int64', 'float64'] and dados_cruzados[col].notna().sum() > 0:
                                contexto.append(f"{col} (m√©dia): {dados_cruzados[col].mean():.2f}")
                    
                    # Colunas de esfor√ßo/complexidade/story points
                    esforco_cols = [col for col in dados_cruzados.columns if 'esforco' in col.lower() or 'points' in col.lower() or 'complex' in col.lower()]
                    if esforco_cols:
                        for col in esforco_cols[:2]:
                            if dados_cruzados[col].dtype in ['int64', 'float64'] and dados_cruzados[col].notna().sum() > 0:
                                contexto.append(f"{col} (m√©dia): {dados_cruzados[col].mean():.2f}")
        
        # Se ainda n√£o houver contexto, adicionar uma mensagem padr√£o
        if not contexto:
            contexto.append("Dados insuficientes para an√°lise detalhada.")
            contexto.append("Sugiro validar os arquivos de entrada ou realizar nova importa√ß√£o.")
            
        # Adicionar sempre uma base de an√°lise com termos t√©cnicos relevantes
        contexto.append("=== Conceitos em Business Agility ===")
        
        # Adicionar explica√ß√£o sobre os termos para enriquecer o contexto
        contexto.append("Lead Time: tempo desde a cria√ß√£o da demanda at√© sua entrega.")
        contexto.append("Cycle Time: tempo efetivo de trabalho em uma demanda.")
        contexto.append("WIP (Work in Progress): quantidade de itens sendo trabalhados simultaneamente.")
        contexto.append("Throughput: taxa de entrega de itens por per√≠odo.")
        contexto.append("Flow Efficiency: porcentagem do tempo total em que o item efetivamente teve trabalho realizado.")
        contexto.append("Organizational Topologies: estrutura organizacional que influencia a efici√™ncia dos times.")
          # Adicionar estrutura organizacional se dispon√≠vel
        org_structure = dados.get('estrutura_organizacional', {})
        if isinstance(org_structure, dict) and org_structure:
            contexto.append("=== Estrutura Organizacional ===")
            
            # Verificar informa√ß√µes sobre tribos na estrutura
            if 'tribos' in org_structure and isinstance(org_structure['tribos'], dict):
                tribos_info = org_structure['tribos']
                # Se a entidade √© uma tribo, adicionar informa√ß√µes espec√≠ficas
                if is_tribo and entidade.lower() in [t.lower() for t in tribos_info]:
                    for tribo_nome, tribo_info in tribos_info.items():
                        if unidecode(tribo_nome.lower()) == nome_norm:
                            if isinstance(tribo_info, dict):
                                for key, value in tribo_info.items():
                                    if key != 'squads':  # Tratamos squads separadamente
                                        contexto.append(f"{key}: {value}")
                                
                                # Adicionar informa√ß√µes sobre squads da tribo
                                if 'squads' in tribo_info and isinstance(tribo_info['squads'], list):
                                    squads_da_tribo = tribo_info['squads']
                                    contexto.append(f"Squads na tribo: {len(squads_da_tribo)}")
                                    if len(squads_da_tribo) <= 10:
                                        contexto.append(f"Lista de squads: {', '.join(squads_da_tribo)}")
                                    else:
                                        contexto.append(f"Amostra de squads: {', '.join(squads_da_tribo[:5])}")
                            break
                else:
                    # Informa√ß√µes gerais de tribos
                    contexto.append(f"N√∫mero de tribos na organiza√ß√£o: {len(tribos_info)}")
                    
            # Verificar informa√ß√µes sobre pap√©is na organiza√ß√£o
            if 'roles' in org_structure and isinstance(org_structure['roles'], dict):
                roles_info = org_structure['roles']
                contexto.append(f"Pap√©is na organiza√ß√£o: {', '.join(roles_info.keys())}")
        
        # Log de diagn√≥stico para verificar o que est√° sendo fornecido para a consulta
        logging.debug(f"Entidade: {entidade}, Tipo identificado: {tipo_entidade}, Nome normalizado: {nome_norm}")
        logging.debug(f"N√∫mero de itens no contexto: {len(contexto)}")
        
        # Gerar resposta com OpenAI
        prompt = f"""
        Consulta: {query}
        
        Contexto:
        {chr(10).join(contexto)}
        
        Dados sobre entidade: {entidade}
        Tipo de entidade: {tipo_entidade if tipo_entidade else "n√£o identificado"}
        
        Por favor, forne√ßa uma resposta objetiva e profissional baseada no contexto acima.
        Se os dados forem insuficientes, indique isso claramente e sugira o que poderia ser feito para melhorar a qualidade da an√°lise.
        
        Formate sua resposta em t√≥picos quando houver m√∫ltiplos aspectos a serem destacados.
        Destaque insights relevantes sobre a entidade analisada.
        """
        
        try:
            completion = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "Voc√™ √© um consultor especialista em Business Agility e an√°lise de dados de equipes √°geis"},
                    {"role": "user", "content": prompt}
                ]
            )            
            return completion.choices[0].message.content
        
        except Exception as e:
            logging.error(f"Erro ao gerar resposta com API: {str(e)}")
            return f"An√°lise indispon√≠vel no momento. Erro na integra√ß√£o com IA: {str(e)}"
        
    except Exception as e:
        logging.error(f"Erro ao gerar resposta: {str(e)}")
        tb_str = traceback.format_exc()
        logging.error(f"Stack trace: {tb_str}")
        return f"Desculpe, n√£o foi poss√≠vel gerar uma resposta: {str(e)}"

def normalizar_texto(texto):
    """Normaliza texto removendo acentos, espa√ßos extras e convertendo para min√∫sculo"""
    if pd.isna(texto):
        return ""
    texto = str(texto).lower().strip()
    # Remover acentos
    texto = unidecode(texto)
    # Remover caracteres especiais
    texto = re.sub(r'[^a-z0-9\s]', '', texto)
    # Substituir m√∫ltiplos espa√ßos por um √∫nico
    texto = re.sub(r'\s+', ' ', texto)
    return texto.strip()

def normalizar_nome(nome):
    """Alias para normalizar_texto para manter compatibilidade com c√≥digo existente"""
    return normalizar_texto(nome)
